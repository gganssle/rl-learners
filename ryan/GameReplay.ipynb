{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Solver!\n",
    "\n",
    "\n",
    "## Intro\n",
    "Flappy bird is a rediculously difficult game with lots and lots of failure and some questionable collision detection.  Perhaps a regrettable choice as a learning experiment.\n",
    "![what the?](./assets/collision.png \"questionable collision\")\n",
    "\n",
    "I followed a few blogs and notes to implement the reinforcement learning agents, particularly:\n",
    "*https://keon.io/deep-q-learning/\n",
    "*https://www.intelnervana.com/demystifying-deep-reinforcement-learning/\n",
    "\n",
    "My goal was to implement deep reinforcement learning for Flappy Bird\n",
    "\n",
    "## Signal to Noise\n",
    "As noted above, flappy bird is extremely difficult.  Flying through even one pipe is extremely unlikely.  Also, the game rewards halfway through a pipe, while there is an extremely high chance of crashing before exiting the pipe.  Thus, my primary issue was getting as much signal as possible out of the system.\n",
    "\n",
    "To that end, I avoided the standard random sampling of states and rewards, and instead opted to randomly sample from full game instances.\n",
    "\n",
    "Furthermore, the input into the agent is both the *state* of the game and the **chosen action**.  The result of the input is the presumed **value** of the state + action.\n",
    "\n",
    "## Method\n",
    "The \"values\" are computed via the *bellman equation*:\n",
    "\n",
    "  Q(s,a) =  r + g * max(Q(s',a'))\n",
    "\n",
    "Where r is the instantaneous reward, g is an internal paramter describing the correlation between this state and the next, while s' represents the next state.\n",
    "\n",
    "Full game histories are fed to the agent and stored in memory.  During training time, a set of histories are extracted at random from the agent's memory.  The histories are **reversed** to make computing Q(s,a) more intuitive.\n",
    "\n",
    "Finally, to help bootstrap the agent, a random *epsilon* factor is introduced to use a random number generator to decide what action to take.  *Epsilon* starts at a high value and is slowly decremented as the model is trained.\n",
    "\n",
    "## Results\n",
    "At the time of writing this, my bird flies through *median* 10 pipes, with a high score of **94** pipes.  Pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import pygame\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Conv3D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "import keras\n",
    "\n",
    "def make_batch(frame):\n",
    "    return frame.reshape(1,frame.shape[0])\n",
    "\n",
    "def prepQ(frame, a):\n",
    "    return np.array([*frame,a])\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, dim):\n",
    "        #paramters\n",
    "        self.gamma = 0.97\n",
    "        self.eps = 0.9\n",
    "        self.eps_dec = 0.995\n",
    "        self.eps_min = 0.0\n",
    "        \n",
    "        #build model(s)\n",
    "        self.model = self.build_model()\n",
    "                \n",
    "        self.batch_size = 64\n",
    "        self.memory_size = self.batch_size * 10\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256,\n",
    "                       input_shape=(9,),  #state is 8, action is 1\n",
    "#                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activation='relu'))\n",
    "        model.add(Dense(256,\n",
    "#                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                       activation='relu'))\n",
    "        model.add(Dense(1,\n",
    "                       activation='linear')) #output is value of state+action pair\n",
    "    \n",
    "        model.compile(loss='mse',optimizer=Adam(lr=0.0001))\n",
    "        return model\n",
    "        \n",
    "    def act(self, state, verbose=False):\n",
    "        rs = np.random.rand();\n",
    "        \n",
    "        if rs < self.eps:\n",
    "            return int(np.random.rand() < 0.5)\n",
    "        \n",
    "        #cheat!\n",
    "        if rs < self.eps:\n",
    "            if state[4] < state[7]:\n",
    "                pad = 65\n",
    "            else:\n",
    "                pad = 65\n",
    "\n",
    "            if (state[4]-pad+state[1]) > state[0]:\n",
    "                act = 1\n",
    "            else:\n",
    "                act = 0\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"cheating\",state, pad, act)                \n",
    "\n",
    "            return act\n",
    "        \n",
    "        flap = self.model.predict(make_batch(prepQ(state,0)))\n",
    "        fall = self.model.predict(make_batch(prepQ(state,1)))\n",
    "        act = int(flap < fall)\n",
    "        if verbose:\n",
    "            print(\"act:\", act, \"Flap:\",flap,\"Fall:\",fall)\n",
    "\n",
    "        return act\n",
    "        \n",
    "    def store_reverse_game(self, history):\n",
    "        history.reverse()\n",
    "        self.memory.append(history)\n",
    "            \n",
    "    def train(self, verbose=False):\n",
    "        gamebatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states = []\n",
    "        qvals = []\n",
    "        \n",
    "        for game in gamebatch:\n",
    "            rr = 0.0\n",
    "            for s,a,r in game:\n",
    "                t = r + self.gamma * rr\n",
    "                states.append(prepQ(s,a))\n",
    "                qvals.append(t)\n",
    "                rr = t\n",
    "        \n",
    "        self.model.fit(np.array(states), np.array(qvals), epochs=1, verbose=verbose)\n",
    "        self.eps = max(self.eps_min, self.eps * self.eps_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a game\n",
    "Agent = QAgent\n",
    "game = FlappyBird()\n",
    "def get_state():\n",
    "    return np.array([float(v) for v in game.getGameState().values()])\n",
    "env = PLE(game, fps=30, force_fps=30, display_screen=True, reward_values={\"positive\": 1.0, \"loss\": -1.0, \"tick\": 0.0,})\n",
    "agent = Agent(6)\n",
    "actions = env.getActionSet()\n",
    "env.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate some memory\n",
    "for i in range(agent.batch_size):\n",
    "    history = []\n",
    "    while not env.game_over():\n",
    "        state = get_state()\n",
    "        act = agent.act(state)\n",
    "        reward = env.act(actions[act])\n",
    "        history.append([state, act, reward])\n",
    "    agent.store_reverse_game(history)\n",
    "    env.reset_game()\n",
    "    env.act(actions[0])\n",
    "agent.train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train in an infinite loop\n",
    "high_score = 0\n",
    "print(\"Starting game with gamma:\",agent.gamma, \"eps:\", agent.eps, \"decay\", agent.eps_dec)\n",
    "try:\n",
    "    while True:\n",
    "        frameset = []\n",
    "        for i in range(agent.batch_size):\n",
    "            history = []\n",
    "            frames = 0\n",
    "            score = 0.0\n",
    "\n",
    "            while not env.game_over():\n",
    "                state = get_state()\n",
    "                act = agent.act(state)\n",
    "                reward = env.act(actions[act])\n",
    "                score += reward\n",
    "                frames += 1\n",
    "                history.append([state,act,reward])\n",
    "\n",
    "            agent.store_reverse_game(history)\n",
    "            env.reset_game()\n",
    "            env.act(actions[0])\n",
    "            frameset.append(frames)\n",
    "\n",
    "            if (frames > high_score):\n",
    "                print(\"New max flight!\", frames, score)\n",
    "                high_score = frames\n",
    "                \n",
    "        print(\"Max frames:\", np.max(frameset), \"Avg frames:\", np.average(frameset), \"Median frames:\", np.median(frameset), \"eps:\", agent.eps)\n",
    "        #hmmm... multi train?\n",
    "        for i in range(5):\n",
    "            agent.train(False)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qc check single game with no random elements\n",
    "e = agent.eps\n",
    "print(\"Note: gamma\", agent.gamma, \"eps\", agent.eps, \"eps decay\", agent.eps_dec)\n",
    "agent.eps = 0\n",
    "score = 0.0\n",
    "for i in range(1):\n",
    "    env.reset_game()\n",
    "    ctr = 0\n",
    "    env.act(actions[0]) #first action\n",
    "    while not env.game_over():\n",
    "        state = get_state()\n",
    "        choice = agent.act(state, verbose=True)\n",
    "        reward = env.act(actions[choice])\n",
    "        score += reward\n",
    "        time.sleep(1./30)\n",
    "        ctr+=1\n",
    "    print(\"flew\",ctr,\"frames, score:\", score)\n",
    "env.reset_game()\n",
    "agent.eps = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "agent.model.save(\"modelv1\")\n",
    "agent.model.save_weights(\"mweightsv1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
